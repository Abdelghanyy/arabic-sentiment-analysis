{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6e8808f17d9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpylab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMLPRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pylab\n",
    "from matplotlib import pyplot\n",
    "import matplotlib as mpl\n",
    "from pylab import *\n",
    "from numpy import *\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Dataset\n",
    "## 1- The First Dataset\n",
    "### The dataset is a folder contains two subfolders, which are positive and negative tweets.Each subfolder contains 1000 tweets, which are numbered in ascending order. \"reading_datast\" starts with initializing two empty arrays, Tarining_dataset and Training_labels.After that, there is a for loop for each subfolder which loops over the files.Reading is done using two built-in functions, The First one is openfile(filename, type) which save the file into a variable called \"file\". The second one is readlines(), which reads all the lines in the variable file and save it into a variable called \"lines\" which converted to a string and saved in a new variable \"tweet\".Another built-in function is used to remove redundant expressions.The last operation in this function is to append the actual tweet to \"training_dataset\" array, which is a substring from char 2 till the end except for the last 4 characters.The last 4 caharacters contains the lable of the tweet and was appended to \"training_labels\" array.\n",
    "\n",
    "\n",
    "## reading_datast(void)--> returns tweets(array),labels(array). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reading_datast():\n",
    "    training_labels=[]\n",
    "    training_dataset=[]\n",
    "\n",
    "    for x in range(1,1001):\n",
    "        filename=\"twitter/positive/positive\"+str(x)+\".txt\"\n",
    "        file = open(filename, 'r')\n",
    "        lines = file.readlines()\n",
    "        tweet=str(lines)\n",
    "        new_tweet=tweet.replace(\"\\\\xa0\",\"\")\n",
    "        training_dataset.append(new_tweet[2:len(new_tweet)-4])\n",
    "        training_labels.append(\"POS\")\n",
    "    for j in range(1,1001):\n",
    "        if(j not in [103,116,176,178,180,184,186,189,191]):\n",
    "            filename=\"twitter/negative/negative\"+str(j)+\".txt\"\n",
    "            file = open(filename, 'r')\n",
    "            lines = file.readlines()\n",
    "            tweet=str(lines)\n",
    "            new_tweet=tweet.replace(\"\\\\xa0\",\"\")\n",
    "            if(j in range(800)):\n",
    "                training_dataset.append(new_tweet[2:len(new_tweet)-4])\n",
    "                training_labels.append(\"NEG\")\n",
    "    return training_dataset,training_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- ASTD:\n",
    "### Reading_new_dataset function is for reading ASTD, which starts with initializing labels and dataset arrays.It also uses the built-in function openfile(filename,type) and saves the file in the \"file\" variable. After that, it loops over the lines and substring the line and appends the last 4 characters to the\"labels\" array. \n",
    "### There is a check to decide whether the label is neutral or not. If the label is \"RAL\" which means neutral then we append the whole line except for the last 8 characters, because the label is written as neutral not RAL which removes the \"NEU\" chars.If it is positive or negative or objective the whole line is added except for the last 5 chars, which contains the label itself and redundant expressions.There is another check if the label is equal to \"OBJ\", it will be changed to \"RAL\". \n",
    "\n",
    "## reading_new_dataset(filename(string))--> returns tweets(array),labels(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reading_new_dataset(filename):\n",
    "    labels = []\n",
    "    dataset=[]\n",
    "    file = open(filename, 'r')\n",
    "    i=0\n",
    "    for line in file :\n",
    "        labels.append(line[-4:])\n",
    "        labels[i]=labels[i][:3]\n",
    "        if (labels[i]==\"RAL\"):\n",
    "            dataset.append(line[:len(line)-8])\n",
    "        else:\n",
    "            dataset.append(line[:len(line)-5])\n",
    "            if (labels[i]==\"OBJ\"):\n",
    "                labels[i]=\"RAL\"\n",
    "        i=i+1\n",
    "\n",
    "    return dataset,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Stop Words.\n",
    "### An empty array \"dataset\" is initialized at the start of reading_stopwords. After that, the built-in function (\"open(filename,type,encoding)\") with the filename that was sent when reading_stopwors was called and encoding UTF-8.\n",
    "### Then, there is a loop over the lines that appends each line of the stop words to the main array \"dataset\".\n",
    "\n",
    "## reading_stopwords(filename(String))--> returns dataset(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reading_stopwords(filename):\n",
    "    dataset=[]\n",
    "    file = open(filename, 'r',encoding='utf-8')\n",
    "    i=0\n",
    "    for line in file :\n",
    "        dataset.append(line[:-1])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datset Manipulation\n",
    "### Reading_new_dataset function is for reading ASTD, which starts with initializing labels and dataset arrays.It also uses the built-in function openfile(filename,type) and saves the file in the \"file\" variable. After that, it loops over the lines and substring the line and appends the last 4 characters to the\"labels\" array. \n",
    "### There is a check to detrmine  whether the label is objective is will be appended to labels array as it, anything else will be appended as \"subjective\".\n",
    "\n",
    "## reading_and_manipulation(filename(string))--> returns tweets(array),labels(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reading_and_manipulation(filename):\n",
    "    labels = []\n",
    "    dataset=[]\n",
    "    file = open(filename, 'r')\n",
    "    i=0\n",
    "    for line in file :\n",
    "        line=file.readline()\n",
    "        labels.append(line[-4:])\n",
    "        labels[i]=labels[i][0:3]\n",
    "        if (labels[i]==\"OBJ\"):\n",
    "            labels[i]=\"OBJ\"\n",
    "        else:\n",
    "            labels[i]=\"SUB\"\n",
    "            dataset.append(line[0:len(line)-5])\n",
    "        i=i+1\n",
    "    return dataset,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Remove Hashtags\n",
    "### Remove_hashtags starts with initializing an array \"words_without_hashtags\", then it loops over an array of words which has been called with. There are multiple checks here, the first one checks whether the word contains a hashtag or not.If there is a hashtag, the built- in function split(argument) will be used, which returns a list of strings without the argument and saved in an array \"hashtag_word\".\n",
    "### The second check will be on \"hashtag_word\" , if the second element contains \"_\" or not. If it contains \"_\" another split will be done and saved in \"two_words\" array to loop over them and each word will be added to the main array \"words_without_hashtags\". Else, if it does not contain the \"_\" then the second item will be just one word and will be added to automatically  to the main array \"words_without_hashtags\".\n",
    "\n",
    "## remove_hashtag(words(array))--> returns words_woithout_hashtag(array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_hashtag (words):\n",
    "    words_without_hashtags=[]\n",
    "    for word in words:\n",
    "        if \"#\" in word:\n",
    "            hashtag_word=word.split(\"#\")\n",
    "            if \"_\" not in hashtag_word[1]:\n",
    "                words_without_hashtags.append(hashtag_word[1])\n",
    "            else:\n",
    "                two_words=hashtag_word[1].split(\"_\")\n",
    "                for new_word in two_words:\n",
    "                    words_without_hashtags.append(new_word)\n",
    "        else:\n",
    "            words_without_hashtags.append(word)\n",
    "    return words_without_hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Punctuation\n",
    "### Remove_punctutaion starts with initializing an array \"new_words_without_punctuation\" .Then, it loops over an array of words that it called with to determine if a word contains a \",\" or not. If a word contains it, it calls the built-in function \"split(\",\")\" and saved the returned list of a word without punctuation in an array \"new_words\". \n",
    "### Another check aims to check if \",\" comes before the word or after it by check the length of the first element of new_word. If the length is greater than 1 , then the second element will be added to the main array \"new_words_without_punctuation\". Otherwise, it means that it comes after the word itself, then the first element will be chosen to be added to the main array.If the word does not contain \",\" at all, it will be added directly to the main array. \n",
    "\n",
    "## remove_punctuation(words(array))--> returns new_words_without_punctuation(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    new_words_without_punctuation=[]\n",
    "    for word in words:\n",
    "        if \"،\" in word:\n",
    "            new_word=word.split(\"،\")\n",
    "            if(len(new_word[0]) > 1):\n",
    "                new_words_without_punctuation.append(new_word[0])\n",
    "            else:\n",
    "                new_words_without_punctuation.append(new_word[1])\n",
    "        else:\n",
    "            new_words_without_punctuation.append(word)\n",
    "    \n",
    "    return new_words_without_punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Tweet\n",
    "### Clean_tweet takes as an input the whole tweet and then returned the words of the tweets without hashtags or punctuation.First of all, it split the whole tweet by calling the built-in function \"split(\"spaces\")\", which splits the tweet by spaces and save the returned list in an array \"remove_spaces\" . \n",
    "### After that, remove_hashtags function is called with \"remove_spaces\" and the returned output saved in an array \"new_wrods_without_hashtags\".The final step is sending the \"new_words_without_hashtags\" to remove_punctuation function and save the returned output in an array \"cleaned\", which is the main array.\n",
    "\n",
    "## Clean_tweet(tweet(string))-->returns(cleaned_tweet(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    remove_spaces=tweet.split(\" \")\n",
    "    new_words_without_hashtags=remove_hashtag(remove_spaces)\n",
    "    cleaned=remove_punctuation(new_words_without_hashtags)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Calculation\n",
    "\n",
    "frequency_calc a function that takes tweets as an input and starts with initializing an empty dictionary, which will contain the frequency of each word.Then, it continues with looping over the tweets and calls clean_tweet for each tweet and save the returned array in array \"cleaned\".After that, it loops over the \"cleaned\" array to determine whether each the \"key\" dictionary contains this word or not. If it contains the word, it will increment the number of occurrences of this word and breaks the loop over the divisionary.\n",
    "Otherwise, it will add it to the dictionary with value equals to 1.\n",
    "\n",
    "## frequency_calc(tweets(array))--> returns(words(dictionary))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frequency_calc (tweets):\n",
    "    words={}\n",
    "    for tweet in tweets:\n",
    "        cleaned_tweet=clean_tweet(tweet)\n",
    "        for word in cleaned_tweet:\n",
    "            flag=True\n",
    "            for key in words:\n",
    "                if word==key:\n",
    "                    words[key]+=1\n",
    "                    flag=False\n",
    "                    break\n",
    "            if flag:\n",
    "                words[word]=1\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frequently_used (words):\n",
    "    sorted_words = [(k, words[k]) for k in sorted(words, key=words.get, reverse=True)]\n",
    "    for k, v in sorted_words:\n",
    "        k, v\n",
    "    return sorted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With_stopwords\n",
    "with_stopwords it takes tweets as an input, then it starts with calling frequency_calc and saves the returned output in a dictionary \"words\".\n",
    "After that, it calls \"frequently_used\" with \"words\" and saves the returned output in an array \"sorted_words\", which will be returned.\n",
    "\n",
    "## with_stopwords(tweets(array))--> returns(sorted_bigram_words(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def with_stopwords(tweets):\n",
    "    words=frequency_calc(tweets)\n",
    "    sorted_words=frequently_used(words)\n",
    "    return sorted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without_Stopwords\n",
    "\n",
    "wihout_stopwords it also takes tweets as an input, then it starts with calling frequency_calc and saves the returned output in a dictionary \"words\", Yet, there is a difference here. It calls reading_stopwords and saves the returned output in an array \"stopwords\". Then, it loops over the dictionary\"words\" and checks whether it contains a stop word or not. If it contains a stop word, it changes its value to -1. Then it calls frequently_used with the modified array. and save the returned output in sorted_words, which will be returned.\n",
    "\n",
    "## without_stopwords(tweets(array))--> returns(sorted_words_without_stop_words(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def without_stopwords(tweets):\n",
    "    words=frequency_calc(tweets)\n",
    "    stopwords=reading_stopwords(\"/Users/ahmed/Desktop/Bachelor/stopwords.txt\")\n",
    "    for word in words:\n",
    "        if word in stopwords:\n",
    "            words[word]=-1\n",
    "    sorted_words=frequently_used(words)\n",
    "    return sorted_words\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Words\n",
    "\n",
    "The two_gram function starts with initializing an empty dictionary \"words\" which will be used to store the bigram words with its frequencies.It takes as an input the whole tweets and starts looping over them and calls clean_tweet function for each tweet of them and store the returned cleaned tweet in an array \"cleaned_tweet\".\n",
    "It continues to loop over the \"cleaned_tweet\" array.Then, it takes each word with its neighbor and saves the bigram words in an array \"two_gram_tweet\".After that, it loops over the dictionary to count occurrences of each bigram word.\n",
    "## two_gram(tweets(array))--> returns(sorted_bigram_words(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_gram(tweets):\n",
    "    words={}\n",
    "    for tweet in tweets:\n",
    "        cleaned_tweet=clean_tweet(tweet)\n",
    "        two_gram_tweet=[]\n",
    "        for x in range(len(cleaned_tweet)-1):\n",
    "            two_gram_tweet.append(cleaned_tweet[x]+\" \"+cleaned_tweet[x+1])\n",
    "        for word in two_gram_tweet:\n",
    "            flag=True\n",
    "            for key in words:\n",
    "                if word==key:\n",
    "                    words[key]+=1\n",
    "                    flag=False\n",
    "                    break\n",
    "            if flag:\n",
    "                words[word]=1\n",
    "                sorted_words=frequently_used(words)\n",
    "    return sorted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram Words\n",
    "\n",
    "The three_gram function starts with initializing an empty dictionary \"words\" which will be used to store the trigram words with its frequencies.It takes as an input the whole tweets and starts looping over them and calls clean_tweet function for each tweet of them and store the returned cleaned tweet in an array \"cleaned_tweet\".\n",
    "It continues to loop over the \"cleaned_tweet\" array.Then, it takes each word with its neighbors and saves the trigram words in an array \"three_gram_tweet\".After that, it loops over the dictionary to count occurrences of each trigram word.\n",
    "\n",
    "## Three_gram(tweets(array))--> returns(sorted_trigram_words(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def three_gram(tweets):\n",
    "    words={}\n",
    "    for tweet in tweets:\n",
    "        cleaned_tweet=clean_tweet(tweet)\n",
    "        three_gram_tweet=[]\n",
    "        for x in range(len(cleaned_tweet)-1):\n",
    "            if((x+2) in range (len(cleaned_tweet))):\n",
    "                three_gram_tweet.append(cleaned_tweet[x]+\" \"+cleaned_tweet[x+1]+\" \"+cleaned_tweet[x+2])\n",
    "\n",
    "        for word in three_gram_tweet:\n",
    "            flag=True\n",
    "            for key in words:\n",
    "                if word==key:\n",
    "                    words[key]+=1\n",
    "                    flag=False\n",
    "                    break\n",
    "            if flag:\n",
    "                words[word]=1\n",
    "    sorted_words=frequently_used(words)\n",
    "    return sorted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "The feature_extraction function takes tweets, words and a limit to the words as inputs.It starts to take the limited words and save it in \"most_frequently_used\" array. Then, an empty array is initialized \"features\" which is an array of arrays of booleans. Then, it continues with looping over the tweets and determine if a word in the tweet or not and if its length is bigger than 1 or not to reduce the number of redundant expressions. If the word in the tweet a true boolean flag is added to the flags array. After finishing the tweet's loop, its boolean flags is added to features.\n",
    "\n",
    "### feature_extraction(tweets(array),words(dictionary),limit(int))-->returns features((an array of arrays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction(tweets,words,limit):\n",
    "    most_frequently_used=words[:limit]\n",
    "    features=[]\n",
    "    for tweet in tweets:\n",
    "        flags=[]\n",
    "        for word in most_frequently_used:\n",
    "            if len(word[0]) >1 and word[1] != -1 : \n",
    "                if word[0] in tweet :\n",
    "                    flags.append(1)\n",
    "                else:\n",
    "                    flags.append(0)\n",
    "        features.append(flags)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup():\n",
    "    new_dataset=reading_new_dataset(\"/Users/ahmed/Desktop/Bachelor/ASTD-master/data/tweets.txt\")\n",
    "    new_tweets=new_dataset[0]\n",
    "    new_lables=new_dataset[1]\n",
    "    words_without=without_stopwords(new_tweets)\n",
    "    print(\"words_without\")\n",
    "    words=with_stopwords(new_tweets)\n",
    "    print(\"words\")\n",
    "    two_gram_words=two_gram(new_tweets)\n",
    "    print(\"two_gram\")\n",
    "    three_gram_words=three_gram(new_tweets)\n",
    "    print(\"three_gram\")\n",
    "    words_two_gram=words+two_gram_words\n",
    "    words_three_gram=words+three_gram_words\n",
    "    words_all=words+two_gram_words+three_gram_words\n",
    "    \n",
    "    words_without_two_gram=words_without+two_gram_words\n",
    "    words_without_three_gram=words_without+three_gram_words\n",
    "    words_without_all=words_without+two_gram_words+three_gram_words\n",
    "    \n",
    "    \n",
    "    features_words=feature_extraction(new_tweets,words,len(words))\n",
    "    print(\"features_words\")\n",
    "    \n",
    "    features_words_without=feature_extraction(new_tweets,words_without,len(words_without))\n",
    "    print(\"features_words_without\")\n",
    "    \n",
    "    features_two_gram_words=feature_extraction(new_tweets,two_gram_words,len(two_gram_words))\n",
    "    print(\"features_two_gram_words\")\n",
    "    \n",
    "    features_three_gram_words=feature_extraction(new_tweets,three_gram_words,len(three_gram_words))\n",
    "    print(\"features_three_gram_words\")\n",
    "    \n",
    "    features_words_two_gram=feature_extraction(new_tweets,words_two_gram,len(words_two_gram))\n",
    "    print(\"features_words_two_gram\")\n",
    "    \n",
    "    features_words_three_gram=feature_extraction(new_tweets,words_three_gram,len(words_three_gram))\n",
    "    print(\"features_words_three_gram\")\n",
    "    \n",
    "    features_words_all=feature_extraction(new_tweets,words_all,len(words_all))\n",
    "    print(\"features_words_all\")\n",
    "\n",
    "    features_words_without_two_gram=feature_extraction(new_tweets,words_without_two_gram,len(words_without_two_gram))\n",
    "    print(\"features_words_without_two_gram\")\n",
    "    \n",
    "    features_words_without_three_gram=feature_extraction(new_tweets,words_without_three_gram,len(words_without_three_gram))\n",
    "    print(\"features_words_without_three_gram\")\n",
    "    \n",
    "    features_words_all=feature_extraction(new_tweets,words_without_all,len(words_without_all))\n",
    "    print(\"features_words_all\")\n",
    "\n",
    "    return new_tweets,new_lables,words_without,words,two_gram_words,three_gram_words,features_words,features_words_without,features_two_gram_words,features_three_gram_words,features_words_two_gram,features_words_three_gram,features_words_all,features_words_without_two_gram,features_words_without_three_gram,features_words_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_without\n",
      "words\n"
     ]
    }
   ],
   "source": [
    "sets=setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    new_tweets=sets[0]\n",
    "    new_labels=sets[1]\n",
    "    words_without=sets[2]\n",
    "    words=sets[3]\n",
    "    two_gram_words=sets[4]\n",
    "    three_gram_words=sets[5]\n",
    "    features_words=sets[6]\n",
    "    features_words_without=sets[7]\n",
    "    features_two_gram_words=sets[8]\n",
    "    features_three_gram_words=sets[9]\n",
    "    features_words_two_gram=sets[10]\n",
    "    features_words_three_gram=sets[11]\n",
    "    features_words_all=sets[12]\n",
    "    features_words_without_two_gram=sets[13]\n",
    "    features_words_without_three_gram=sets[14]\n",
    "    features_words_all=sets[15]\n",
    "    file=open(\"gaussian.txt\",\"w\")\n",
    "    \n",
    "    \n",
    "    words_gaussian_score=gaussian_class(features_words,new_labels)\n",
    "    file.write(words_gaussian_score+\"words_gaussian_score\\n\")\n",
    "    print(\"1\")\n",
    "    \n",
    "    words_without_gaussian_score=gaussian_class(features_words_without,new_labels)\n",
    "    file.write(words_without_gaussian_score+\"words_without_gaussian_score\\n\")\n",
    "    print(\"2\")\n",
    "    \n",
    "    \n",
    "    two_gram_words_gaussian_score=gaussian_class(features_two_gram_words,new_labels)\n",
    "    file.write(two_gram_words_gaussian_score+\"two_gram_words_gaussian_score\\n\")\n",
    "    print(\"3\")\n",
    "    \n",
    "    three_gram_words_gaussian_score=gaussian_class(features_two_gram_words,new_labels)\n",
    "    file.write(three_gram_words_gaussian_score+\"three_gram_words_gaussian_score\\n\")\n",
    "    print(\"4\")\n",
    "    \n",
    "    words_two_gram_gaussian_score=gaussian_class(features_words_two_gram,new_labels)\n",
    "    file.write(words_two_gram_gaussian_score+\"words_two_gram_gaussian_score\\n\")\n",
    "    print(\"5\")\n",
    "    \n",
    "    words_three_gram_words_gaussian_score=gaussian_class(features_words_three_gram,new_labels)\n",
    "    file.write(words_three_gram_words_gaussian_score+\"words_three_gram_words_gaussian_score\\n\")\n",
    "    print(\"6\")\n",
    "    \n",
    "    words_all_gaussian_score=gaussian_class(features_words_all,new_labels)\n",
    "    file.write(words_all_gaussian_score+\"words_all_gaussian_score\\n\")\n",
    "    print(\"7\")\n",
    "    \n",
    "    words_without_two_gram_gaussian_score=gaussian_class(features_words_without_two_gram,new_labels)\n",
    "    file.write(words_without_two_gram_gaussian_score+\"words_without_two_gram_gaussian_score\\n\")\n",
    "    print(\"8\")\n",
    "    \n",
    "    words_without_three_gram_gaussian_score=gaussian_class(features_words_without_three_gram,new_labels)\n",
    "    file.write(words_without_three_gram_gaussian_score+\"words_without_three_gram_gaussian_score\\n\")\n",
    "    print(\"9\")\n",
    "    \n",
    "    words_without_all_gaussian_score=gaussian_class(features_words_all,new_labels)\n",
    "    file.write(words_without_all_gaussian_score+\"words_without_all_gaussian_score\\n\")\n",
    "    print(\"10\")\n",
    "    \n",
    "    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-29dd03a410d3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnew_tweets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnew_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwords_without\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sets' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_class(features,labels):\n",
    "    \n",
    "    training_labe=labels[:8000]\n",
    "    testing_labe=labels[-2006:]\n",
    "    training_datas=features[:8000]\n",
    "    testing_datas=features[-2006:]\n",
    "    \n",
    "    gclassfier= GaussianNB()\n",
    "    gclassfier.fit(training_datas,training_labe)\n",
    "    gscore = gclassfier.score(testing_datas, testing_labe)\n",
    "    \n",
    "    return (gscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multinomial_class(features,labels):\n",
    "    \n",
    "    training_labe=labels[:8000]\n",
    "    testing_labe=labels[-2006:]\n",
    "    training_datas=features[:8000]\n",
    "    testing_datas=features[-2006:]\n",
    "    \n",
    "    classfier= MultinomialNB()\n",
    "    classfier.fit(training_datas,training_labe)\n",
    "    score = classfier.score(testing_datas, testing_labe)\n",
    "    \n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bernoulli_class(features,labels):\n",
    "    \n",
    "    training_labe=labels[:8000]\n",
    "    testing_labe=labels[-2006:]\n",
    "    training_datas=features[:8000]\n",
    "    testing_datas=features[-2006:]\n",
    "    \n",
    "    classfier= BernoulliNB()\n",
    "    classfier.fit(training_datas,training_labe)\n",
    "    score = classfier.score(testing_datas, testing_labe)\n",
    "    \n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_class(features,labels):\n",
    "    \n",
    "    training_labe=labels[:8000]\n",
    "    testing_labe=labels[-2006:]\n",
    "    training_datas=features[:8000]\n",
    "    testing_datas=features[-2006:]\n",
    "    \n",
    "    classfier= svm=sklearn.svm.SVC()\n",
    "    classfier.fit(training_datas,training_labe)\n",
    "    score = classfier.score(testing_datas, testing_labe)\n",
    "    \n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomforest_class(features,labels):\n",
    "    \n",
    "    training_labe=labels[:8000]\n",
    "    testing_labe=labels[-2006:]\n",
    "    training_datas=features[:8000]\n",
    "    testing_datas=features[-2006:]\n",
    "    \n",
    "    classfier= RandomForestClassifier()\n",
    "    classfier.fit(training_datas,training_labe)\n",
    "    score = classfier.score(testing_datas, testing_labe)\n",
    "    \n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_NB_univariant(features,labels,number):\n",
    "\n",
    "    new_features = SelectKBest(chi2, k=number).fit_transform(features,labels)\n",
    "    \n",
    "    training_labels=labels[:1598]\n",
    "    testing_labels=labels[-402:]\n",
    "    training_dataset=new_features[:1598]\n",
    "    testing_dataset=new_features[-402:]\n",
    "    #print(testing_labels)\n",
    "    classfier= OneVsRestClassifier(MultinomialNB())\n",
    "    classfier.fit(training_dataset,training_labels)\n",
    "    score = classfier.score(testing_dataset, testing_labels)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
